# Analyze Data with Apache Spark in Microsoft Fabric

This notebook demonstrates how to ingest data into a **Microsoft Fabric Lakehouse** and analyze it using **PySpark**. It is part of a hands-on learning lab focused on building data engineering skills with Apache Spark.

---

## Project Overview

In this lab, you will:

- Ingest data into the **Fabric Lakehouse**
- Use **PySpark** to load and transform data
- Perform basic data analysis using Spark DataFrame API

---

## Technologies Used

- **Apache Spark (PySpark)**
- **Microsoft Fabric (Lakehouse)**
- **Python 3**
- **Jupyter Notebook**

---

## What You’ll Learn

- How to read data from a Fabric Lakehouse using PySpark
- How to apply transformations using the Spark DataFrame API
- How to run queries and derive insights from big data
- How Spark fits into a modern data platform like Microsoft Fabric

---

## Key Takeaways

✅ PySpark enables distributed data processing using Python  
✅ Microsoft Fabric integrates Spark seamlessly for scalable analytics  
✅ Delta Lake format supports versioning, transactions, and streaming

---

## How to Use

1. Open the notebook in your preferred environment (Fabric, Databricks, or local Jupyter)
2. Follow the step-by-step instructions in `spark_analysis.ipynb`
3. Modify the dataset or add new transformations to deepen your understanding

---

## Credits

This project is a part of a data engineering learning lab on Microsoft Fabric.

---

## Contact

Have questions or feedback? Reach out on [LinkedIn](https://linkedin.com) or start a GitHub discussion.

